{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "playground for testing evaluation metrics on consitency with other author (Ben Hammer)\n",
    "and plausibility checks\n",
    "'''\n",
    "import numpy as np\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from \n",
    "# https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "    # print(\"ben Hammer predicted\", predicted)\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:  \n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "            # print(\"ben Hammer add score\", score)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score # / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision my take:  0.5\n",
      "average precision Ben Hammer:  0.5\n",
      "mean average precision my take:  0.416666666667\n",
      "mean average precision Ben Hammer:  0.416666666667\n",
      "average precision with strings:  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "evaluation metrics MAP@5\n",
    "sources: \n",
    "https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
    "https://www.kaggle.com/c/FacebookRecruiting/discussion/2002\n",
    "https://en.wikipedia.org/wiki/Information_retrieval\n",
    "Note, that the metric is designed for \"document retrieval\", \n",
    "where many outcomes might be true (= \"relevant documents\")\n",
    "Our case is specific, as there is only one \"relevant document\" per prediction \n",
    "(= the true prediction)\n",
    "'''\n",
    "\n",
    "# test implementation against Ben Hammers one\n",
    "test = average_precision([1,12,3,12,8],12,5)\n",
    "print(\"average precision my take: \",test)\n",
    "\n",
    "test = apk([12],[1,12,3,12,8],5)\n",
    "print(\"average precision Ben Hammer: \",test)\n",
    "\n",
    "\n",
    "test = mean_average_precision([[1,13,3,12,8], [5,3,12,3,6], [8,6,11,2,4]],[12,15,8],5 )\n",
    "print(\"mean average precision my take: \",test)\n",
    "\n",
    "test = mapk([[12],[15],[8]], [[1,13,3,12,8], [5,3,12,3,6], [8,6,11,8,4]], 5)\n",
    "print(\"mean average precision Ben Hammer: \",test)\n",
    "\n",
    "true_lables = [12,15,8]\n",
    "model_predictions = [[1,13,3,12,8], [5,3,12,3,6], [8,6,11,8,4]]\n",
    "\n",
    "test = average_precision(['ga','gi','go','gu','ge'], 'go', 5)\n",
    "print(\"average precision with strings: \",test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old directory removed data/dumb_train\n",
      "copy 34 images for whale # 1 in ordered list, called w_1287fbc\n",
      "copy 27 images for whale # 2 in ordered list, called w_98baff9\n",
      "copy 26 images for whale # 3 in ordered list, called w_7554f44\n",
      "write csv file: data/dumb_train.csv\n",
      "4251 individuals\n",
      "dummy_preds first 10: \n",
      " [array([  72, 3093, 3448, 2663,  554]), array([2245, 3498, 2332, 2935, 2695]), array([2736, 1627, 4152, 3065, 1350]), array([2923, 1929, 1590, 1756, 3128]), array([1433, 4196,   87, 2150,  457]), array([2374, 3760, 3333, 1425, 4179]), array([ 729,  954, 2395, 1622, 3709]), array([ 860, 2564, 2405, 3753, 3033]), array([3093, 3533, 2155,  172, 3333]), array([ 710, 1760,  280,  444,  917])]\n",
      "true_labels  first 50: \n",
      " [3978, 1, 55, 2234, 1984, 772, 4058, 0, 62, 662, 340, 2957, 796, 7, 689, 828, 406, 3763, 945, 0, 0, 744, 274, 860, 3597, 1175, 0, 664, 1324, 282, 0, 1230, 46, 579, 76, 2915, 1658, 1606, 566, 1237, 0, 183, 816, 2373, 38, 442, 3083, 4097, 1634, 664]\n",
      "\n",
      " MAP 0.000653130287648\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "As a benchmark create case of a dumb model without any predictive power and test, how this model performs\n",
    "measured in MAP@5 metric\n",
    "Test set contains 15,610 images. \n",
    "Assume, that a \"dumb\" model will map the images to the individuals randomly, \n",
    "'''\n",
    "\n",
    "# def test_evaluate(test_csv\"data/small_train.csv\", \"data/small_train\"):\n",
    "create_small_case(sel_whales = [1,2,3],\n",
    "                  small_dir = \"data/dumb_train\", \n",
    "                  small_csv = \"data/dumb_train.csv\")\n",
    " \n",
    "# train_list = read_csv(file_name = \"data/dumb_train.csv\")   # for testing toy data set\n",
    "train_list = read_csv(file_name = \"data/train.csv\")   # for testing whole train data set\n",
    "\n",
    "whales, counts = get_whales(train_list)\n",
    "print(\"{} individuals\".format(len(counts)))  \n",
    "\n",
    "# to each image in train_list map a ranked list of max_pred whales\n",
    "# as random number between 1 and # of individuals in scenario (indeces in whale list)\n",
    "max_pred = 5\n",
    "dummy_preds = []\n",
    "for i in range(len(train_list)):\n",
    "    ranks = np.random.randint(0,len(counts),max_pred)\n",
    "    dummy_preds.append(ranks)\n",
    "\n",
    "# get list of true labels: retrieve whale number from name\n",
    "true_labels = []\n",
    "for i, img in enumerate(train_list):\n",
    "    name = img[1]\n",
    "    true_labels.append([i for i, whale in enumerate(whales) if whale[0] == name][0])\n",
    "\n",
    "print(\"dummy_preds first 10: \\n\",dummy_preds[:10])\n",
    "print(\"true_labels  first 50: \\n\",true_labels[:50])\n",
    "\n",
    "MAP = mean_average_precision(dummy_preds, true_labels, max_pred)\n",
    "print(\"\\n MAP\", MAP)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy.misc import imread\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n"
     ]
    }
   ],
   "source": [
    "# global variables\n",
    "all_train_dir = \"data/train\"     # directory with original kaggle training data\n",
    "all_train_csv = \"data/train.csv\" # original kaggle train.csv file\n",
    "train_dir = \"data/model_train\"\n",
    "train_csv = \"data/model_train.csv\"\n",
    "valid_dir = \"data/model_valid\"\n",
    "valid_csv = \"data/model_valid.csv\"\n",
    "\n",
    "num_classes = 7     # number of whales to be considered (in order of occuurence)\n",
    "max_preds = 5       # number of ranked predictions (default 5)\n",
    "batch_size = 16     # used for training as well as validation\n",
    "train_valid = 0.8   # ratio training / validation data\n",
    "\n",
    "# create training environment for training data\n",
    "num_train_imgs, num_valid_imgs = create_small_case(\n",
    "       sel_whales = np.arange(1,num_classes+1),  # whales to be considered\n",
    "       all_train_dir = all_train_dir,\n",
    "       all_train_csv = all_train_csv,\n",
    "       train_dir = train_dir,\n",
    "       train_csv = train_csv,\n",
    "       valid_dir = valid_dir,\n",
    "       valid_csv = valid_csv,\n",
    "       train_valid = train_valid,\n",
    "       sub_dirs = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)                     # new added as https://towardsdatascience.com/transfer-learning-using-keras-d804b2e04ef8\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "# and a logistic layer\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "# metrics='accuracy' causes the model to store and report accuracy (train and validate)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "{'w_1eafe46': 1, 'w_7554f44': 3, 'w_fd1cb9d': 6, 'w_ab4cae2': 5, 'w_98baff9': 4, 'w_1287fbc': 0, 'w_693c9ee': 2}\n",
      "{0: 'w_1287fbc', 1: 'w_1eafe46', 2: 'w_693c9ee', 3: 'w_7554f44', 4: 'w_98baff9', 5: 'w_ab4cae2', 6: 'w_fd1cb9d'}\n"
     ]
    }
   ],
   "source": [
    "# define image generator\n",
    "train_gen = image.ImageDataGenerator(\n",
    "    # featurewise_center=True,\n",
    "    # featurewise_std_normalization=True,\n",
    "    rescale = 1./255,   # redundant with featurewise_center ? \n",
    "    # preprocessing_function=preprocess_input, not used in most examples\n",
    "    # horizontal_flip = True,    # no, as individual shapes are looked for\n",
    "    fill_mode = \"nearest\",\n",
    "    zoom_range = 0.3,\n",
    "    width_shift_range = 0.3,\n",
    "    height_shift_range=0.3,\n",
    "    rotation_range=30)\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "train_flow = train_gen.flow_from_directory(\n",
    "    train_dir,\n",
    "    # save_to_dir = \"data/model_train/augmented\",    \n",
    "    # color_mode = \"grayscale\",\n",
    "    target_size = (299,299),\n",
    "    batch_size = batch_size, \n",
    "    class_mode = \"categorical\")\n",
    "\n",
    "valid_gen = image.ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    fill_mode = \"nearest\")\n",
    "\n",
    "valid_flow = valid_gen.flow_from_directory(\n",
    "    valid_dir,\n",
    "    target_size = (299,299),\n",
    "    class_mode = \"categorical\")\n",
    "\n",
    "whale_class_map = (train_flow.class_indices)           # get dict mapping whalenames --> class_no\n",
    "class_whale_map = make_label_dict(directory=train_dir) # get dict mapping class_no --> whalenames\n",
    "print(whale_class_map)\n",
    "print(class_whale_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n"
     ]
    }
   ],
   "source": [
    "print(num_train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total. Thereof:\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 155s - loss: 1.4244 - acc: 0.4844 - val_loss: 1.6365 - val_acc: 0.3846\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total. Thereof:\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 141s - loss: 1.5168 - acc: 0.4590 - val_loss: 1.5039 - val_acc: 0.3333\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total. Thereof:\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 153s - loss: 1.1941 - acc: 0.5587 - val_loss: 1.3165 - val_acc: 0.5128\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total. Thereof:\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 140s - loss: 1.3687 - acc: 0.4698 - val_loss: 1.5070 - val_acc: 0.3590\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total. Thereof:\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 138s - loss: 1.1504 - acc: 0.5327 - val_loss: 1.3357 - val_acc: 0.4359\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total. Thereof:\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 139s - loss: 1.3375 - acc: 0.5538 - val_loss: 1.4301 - val_acc: 0.4615\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total. Thereof:\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 134s - loss: 1.1503 - acc: 0.5612 - val_loss: 1.3400 - val_acc: 0.5385\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total. Thereof:\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 144s - loss: 1.1736 - acc: 0.5617 - val_loss: 1.4078 - val_acc: 0.5641\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total. Thereof:\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 149s - loss: 1.2093 - acc: 0.6108 - val_loss: 1.1381 - val_acc: 0.6154\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total. Thereof:\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 145s - loss: 1.3911 - acc: 0.4983 - val_loss: 1.5815 - val_acc: 0.3590\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total. Thereof:\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 145s - loss: 1.0425 - acc: 0.6142 - val_loss: 1.0878 - val_acc: 0.7179\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total. Thereof:\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 145s - loss: 1.0775 - acc: 0.6457 - val_loss: 1.2837 - val_acc: 0.5128\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total. Thereof:\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 145s - loss: 0.9479 - acc: 0.6722 - val_loss: 1.2218 - val_acc: 0.5641\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total. Thereof:\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 145s - loss: 1.5036 - acc: 0.5376 - val_loss: 1.2338 - val_acc: 0.5641\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total. Thereof:\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 145s - loss: 0.9160 - acc: 0.6565 - val_loss: 1.1219 - val_acc: 0.5641\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total. Thereof:\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 145s - loss: 1.1491 - acc: 0.5224 - val_loss: 1.3331 - val_acc: 0.5128\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total. Thereof:\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 145s - loss: 0.9860 - acc: 0.6904 - val_loss: 1.6630 - val_acc: 0.4359\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total. Thereof:\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 144s - loss: 1.2664 - acc: 0.5327 - val_loss: 1.1558 - val_acc: 0.5128\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total. Thereof:\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 146s - loss: 1.0459 - acc: 0.6801 - val_loss: 1.1548 - val_acc: 0.5641\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total. Thereof:\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 145s - loss: 1.2738 - acc: 0.5985 - val_loss: 1.6142 - val_acc: 0.4872\n"
     ]
    }
   ],
   "source": [
    "# train in cross validation loop\n",
    "for i in range(20):\n",
    "    # create new environment with new random train / valid split\n",
    "    num_train_imgs, num_valid_imgs = create_small_case(\n",
    "       sel_whales = np.arange(1,num_classes+1),  # whales to be considered\n",
    "       all_train_dir = all_train_dir,\n",
    "       all_train_csv = all_train_csv,\n",
    "       train_dir = train_dir,\n",
    "       train_csv = train_csv,\n",
    "       valid_dir = valid_dir,\n",
    "       valid_csv = valid_csv,\n",
    "       train_valid = train_valid,\n",
    "       sub_dirs = True) \n",
    "\n",
    "    # define image generator\n",
    "    train_gen = image.ImageDataGenerator(\n",
    "        # featurewise_center=True,\n",
    "        # featurewise_std_normalization=True,\n",
    "        rescale = 1./255,   # redundant with featurewise_center ? \n",
    "        # preprocessing_function=preprocess_input, not used in most examples\n",
    "        # horizontal_flip = True,    # no, as individual shapes are looked for\n",
    "        fill_mode = \"nearest\",\n",
    "        zoom_range = 0.3,\n",
    "        width_shift_range = 0.3,\n",
    "        height_shift_range=0.3,\n",
    "        rotation_range=30)\n",
    "\n",
    "    train_flow = train_gen.flow_from_directory(\n",
    "        train_dir,\n",
    "        # save_to_dir = \"data/model_train/augmented\",    \n",
    "        # color_mode = \"grayscale\",\n",
    "        target_size = (299,299),\n",
    "        batch_size = batch_size, \n",
    "        class_mode = \"categorical\")\n",
    "\n",
    "    valid_gen = image.ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "        fill_mode = \"nearest\")\n",
    "\n",
    "    valid_flow = valid_gen.flow_from_directory(\n",
    "        valid_dir,\n",
    "        target_size = (299,299),\n",
    "        class_mode = \"categorical\") \n",
    "\n",
    "    hist = model.fit_generator(\n",
    "        train_flow, \n",
    "        steps_per_epoch = num_train_imgs//batch_size,\n",
    "        verbose = 2, \n",
    "        validation_data = valid_flow,   # to be used later\n",
    "        validation_steps = num_valid_imgs//batch_size,\n",
    "        epochs=1)              \n",
    "\n",
    "model.save('InceptV3_20_Epochs_7_classes_CV.h5')\n",
    "\n",
    "# here the model started already from pretrained status (initial loss was ~8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old directory removed data/model_test\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_test  validation:  None\n",
      "176  images copied as training data\n",
      "0  images copied as validation data\n",
      "write csv file with training data: data/model_test.csv\n"
     ]
    }
   ],
   "source": [
    "# training seems so work - accuracy of ~0.5 on training and validation data\n",
    "\n",
    "# try to verify on test data --> no success so far\n",
    "\n",
    "# use all training data of the first num_classes (7) whales a test data.\n",
    "# no good practice, but all training data have been augmented, so at least some indication\n",
    "# about predictive power of model\n",
    "test_dir = \"data/model_test\"\n",
    "test_csv = \"data/model_test.csv\"\n",
    "num_train_imgs, num_valid_imgs = create_small_case(\n",
    "       sel_whales = np.arange(1,7+1),  # whales to be considered\n",
    "       all_train_dir = all_train_dir,\n",
    "       all_train_csv = all_train_csv,\n",
    "       train_dir = test_dir,\n",
    "       train_csv = test_csv,\n",
    "       valid_dir = None,     # no validation, copy all data into test_dir \"data/model_test\"\n",
    "       valid_csv = None,\n",
    "       train_valid = 1.,\n",
    "       sub_dirs = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 176 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# for test Purposes !!!\n",
    "\n",
    "# valid_gen = image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_gen = image.ImageDataGenerator(\n",
    "    # featurewise_center=True,\n",
    "    # featurewise_std_normalization=True,\n",
    "    rescale = 1./255,\n",
    "    # preprocessing_function=preprocess_input,   # model specific function\n",
    "    fill_mode = \"nearest\")\n",
    "\n",
    "test_flow = test_gen.flow_from_directory(\n",
    "    test_dir,\n",
    "    # color_mode = \"grayscale\",\n",
    "    batch_size = batch_size,     \n",
    "    target_size = (299,299),\n",
    "    class_mode = \"categorical\")    # use \"None\" ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 191s 17s/step\n",
      "(176, 7)\n",
      "[[  4.63210978e-03   3.00654769e-03   2.08380647e-04   3.28651840e-05\n",
      "    2.13375757e-03   2.42068578e-04   9.89744365e-01]\n",
      " [  4.22893502e-02   1.91685081e-01   1.02025822e-01   1.22443028e-02\n",
      "    2.54318833e-01   3.79027516e-01   1.84090622e-02]\n",
      " [  4.87957336e-02   3.56731176e-01   1.06019750e-01   2.23210137e-02\n",
      "    1.65488258e-01   2.62976795e-01   3.76672558e-02]\n",
      " [  1.11918822e-01   3.65958400e-02   1.60670914e-02   8.33930506e-04\n",
      "    6.80789471e-01   1.00539647e-01   5.32551333e-02]\n",
      " [  1.64128207e-02   3.37458588e-02   4.00195569e-02   4.13647760e-03\n",
      "    7.77367234e-01   1.02771342e-01   2.55466010e-02]\n",
      " [  6.20104000e-03   8.65606666e-01   3.90951335e-03   4.19794087e-04\n",
      "    9.49497707e-03   6.82832021e-03   1.07539691e-01]\n",
      " [  1.24325147e-02   1.17013408e-02   1.69343483e-02   9.31790099e-04\n",
      "    8.58521998e-01   9.30807367e-02   6.39727805e-03]\n",
      " [  2.70989574e-02   5.69240987e-01   7.86153376e-02   7.00570410e-03\n",
      "    1.14280954e-01   1.72118634e-01   3.16394903e-02]\n",
      " [  5.97229116e-02   3.86115313e-02   2.65900511e-02   3.94767337e-03\n",
      "    3.93650353e-01   2.77946275e-02   4.49682921e-01]\n",
      " [  1.75727624e-02   4.44667876e-01   3.92937250e-02   1.31709839e-03\n",
      "    3.76334459e-01   1.05533734e-01   1.52802188e-02]]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict_generator(test_flow, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w_1eafe46': 1, 'w_7554f44': 3, 'w_fd1cb9d': 6, 'w_ab4cae2': 5, 'w_98baff9': 4, 'w_1287fbc': 0, 'w_693c9ee': 2}\n",
      "{0: 'w_1287fbc', 1: 'w_1eafe46', 2: 'w_693c9ee', 3: 'w_7554f44', 4: 'w_98baff9', 5: 'w_ab4cae2', 6: 'w_fd1cb9d'}\n"
     ]
    }
   ],
   "source": [
    "whale_class_map = (test_flow.class_indices)           # get dict mapping whalenames --> class_no\n",
    "class_whale_map = make_label_dict(directory=test_dir) # get dict mapping class_no --> whalenames\n",
    "print(whale_class_map)\n",
    "print(class_whale_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176, 7)\n",
      "[[  4.63210978e-03   3.00654769e-03   2.08380647e-04   3.28651840e-05\n",
      "    2.13375757e-03   2.42068578e-04   9.89744365e-01]\n",
      " [  4.22893502e-02   1.91685081e-01   1.02025822e-01   1.22443028e-02\n",
      "    2.54318833e-01   3.79027516e-01   1.84090622e-02]\n",
      " [  4.87957336e-02   3.56731176e-01   1.06019750e-01   2.23210137e-02\n",
      "    1.65488258e-01   2.62976795e-01   3.76672558e-02]\n",
      " [  1.11918822e-01   3.65958400e-02   1.60670914e-02   8.33930506e-04\n",
      "    6.80789471e-01   1.00539647e-01   5.32551333e-02]\n",
      " [  1.64128207e-02   3.37458588e-02   4.00195569e-02   4.13647760e-03\n",
      "    7.77367234e-01   1.02771342e-01   2.55466010e-02]\n",
      " [  6.20104000e-03   8.65606666e-01   3.90951335e-03   4.19794087e-04\n",
      "    9.49497707e-03   6.82832021e-03   1.07539691e-01]\n",
      " [  1.24325147e-02   1.17013408e-02   1.69343483e-02   9.31790099e-04\n",
      "    8.58521998e-01   9.30807367e-02   6.39727805e-03]\n",
      " [  2.70989574e-02   5.69240987e-01   7.86153376e-02   7.00570410e-03\n",
      "    1.14280954e-01   1.72118634e-01   3.16394903e-02]\n",
      " [  5.97229116e-02   3.86115313e-02   2.65900511e-02   3.94767337e-03\n",
      "    3.93650353e-01   2.77946275e-02   4.49682921e-01]\n",
      " [  1.75727624e-02   4.44667876e-01   3.92937250e-02   1.31709839e-03\n",
      "    3.76334459e-01   1.05533734e-01   1.52802188e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(preds.shape)\n",
    "print(preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model predictions: \n",
      " [['w_fd1cb9d' 'w_1287fbc' 'w_1eafe46' 'w_98baff9' 'w_ab4cae2']\n",
      " ['w_ab4cae2' 'w_98baff9' 'w_1eafe46' 'w_693c9ee' 'w_1287fbc']\n",
      " ['w_1eafe46' 'w_ab4cae2' 'w_98baff9' 'w_693c9ee' 'w_1287fbc']\n",
      " ['w_98baff9' 'w_1287fbc' 'w_ab4cae2' 'w_fd1cb9d' 'w_1eafe46']\n",
      " ['w_98baff9' 'w_ab4cae2' 'w_693c9ee' 'w_1eafe46' 'w_fd1cb9d']\n",
      " ['w_1eafe46' 'w_fd1cb9d' 'w_98baff9' 'w_ab4cae2' 'w_1287fbc']\n",
      " ['w_98baff9' 'w_ab4cae2' 'w_693c9ee' 'w_1287fbc' 'w_1eafe46']\n",
      " ['w_1eafe46' 'w_ab4cae2' 'w_98baff9' 'w_693c9ee' 'w_fd1cb9d']\n",
      " ['w_fd1cb9d' 'w_98baff9' 'w_1287fbc' 'w_1eafe46' 'w_ab4cae2']\n",
      " ['w_1eafe46' 'w_98baff9' 'w_ab4cae2' 'w_693c9ee' 'w_1287fbc']\n",
      " ['w_98baff9' 'w_ab4cae2' 'w_1eafe46' 'w_fd1cb9d' 'w_1287fbc']\n",
      " ['w_ab4cae2' 'w_1eafe46' 'w_98baff9' 'w_693c9ee' 'w_1287fbc']\n",
      " ['w_1eafe46' 'w_98baff9' 'w_693c9ee' 'w_ab4cae2' 'w_1287fbc']\n",
      " ['w_1eafe46' 'w_98baff9' 'w_ab4cae2' 'w_fd1cb9d' 'w_693c9ee']\n",
      " ['w_7554f44' 'w_ab4cae2' 'w_1eafe46' 'w_693c9ee' 'w_98baff9']\n",
      " ['w_1eafe46' 'w_ab4cae2' 'w_98baff9' 'w_693c9ee' 'w_fd1cb9d']\n",
      " ['w_fd1cb9d' 'w_1eafe46' 'w_1287fbc' 'w_98baff9' 'w_693c9ee']\n",
      " ['w_1eafe46' 'w_98baff9' 'w_ab4cae2' 'w_693c9ee' 'w_1287fbc']\n",
      " ['w_ab4cae2' 'w_1eafe46' 'w_98baff9' 'w_693c9ee' 'w_7554f44']\n",
      " ['w_98baff9' 'w_ab4cae2' 'w_1eafe46' 'w_693c9ee' 'w_1287fbc']]\n",
      "true labels \n",
      " ['w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc'\n",
      " 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc'\n",
      " 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc'\n",
      " 'w_1287fbc' 'w_1287fbc']\n"
     ]
    }
   ],
   "source": [
    "# ge list of model predictions: one ordered list of maxpred whalenames per image\n",
    "top_k = preds.argsort()[:, -max_preds:][:, ::-1]\n",
    "# top_k = preds.argsort()[:, -max_preds:]\n",
    "model_preds = [([class_whale_map[i] for i in line]) for line in top_k]  \n",
    "\n",
    "# get list of true labels: one whalename per image\n",
    "# test_list = read_csv(file_name = test_csv)    # list with (filename, whalename)\n",
    "true_labels = []\n",
    "for fn in test_flow.filenames:\n",
    "    offset, filename = fn.split('/')\n",
    "    whale = [line[1] for line in test_list if line[0]==filename][0]\n",
    "    true_labels.append(whale)\n",
    "\n",
    "print(\"model predictions: \\n\", np.array(model_preds)[0:20])\n",
    "print(\"true labels \\n\", np.array(true_labels)[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP 0.321022727273\n",
      "Dummy MAP weighted 0.251325757576\n",
      "Dummy MAP weighted 0.311931818182\n",
      "Dummy MAP weighted 0.283049242424\n",
      "Dummy MAP weighted 0.324621212121\n",
      "Dummy MAP weighted 0.264962121212\n",
      "Dummy MAP weighted 0.273863636364\n",
      "Dummy MAP weighted 0.310700757576\n",
      "Dummy MAP weighted 0.261363636364\n",
      "Dummy MAP weighted 0.317140151515\n",
      "Dummy MAP weighted 0.289583333333\n"
     ]
    }
   ],
   "source": [
    "MAP = mean_average_precision(model_preds, true_labels, max_preds)\n",
    "print(\"MAP\", MAP)\n",
    "\n",
    "for i in range(10):\n",
    "    Dummy_map = Dummy_MAP(probs = 'weighted', distributed_as = train_csv, image_no = len(test_list))\n",
    "    print(\"Dummy MAP weighted\", Dummy_map)\n",
    "\n",
    "# MAP only slightly higher than averag dummy MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
